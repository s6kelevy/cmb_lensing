{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "married-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import random\n",
    "from colossus.cosmology import cosmology\n",
    "cosmology.setCosmology('planck18')\n",
    "from colossus.halo import concentration, mass_defs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc;rc('text', usetex=True);rc('font', weight='bold');matplotlib.rcParams['text.latex.preamble'] = r'\\boldmath'\n",
    "rcParams['font.family'] = 'serif'\n",
    "rc('text.latex',preamble=r'\\usepackage{/Volumes/Extreme_SSD/codes/master_thesis/code/configs/apjfonts}')\n",
    "sz_ft = 20\n",
    "sz_lb = 14\n",
    "color_arr = ['indigo', 'royalblue', 'lightseagreen', 'darkgreen', 'goldenrod', 'darkred']\n",
    "from tqdm import tqdm\n",
    "import lensing_estimator\n",
    "from cosmo import CosmoCalc\n",
    "import lensing\n",
    "import foregrounds as fg\n",
    "import experiments as exp\n",
    "import sims\n",
    "import stats\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-kidney",
   "metadata": {},
   "source": [
    "## Estimator Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "nber_clus = 3000\n",
    "nber_rand = 30000\n",
    "nber_cov = 500\n",
    "map_params = [240, 0.25, 240, 0.25]\n",
    "l, cl = CosmoCalc().cmb_power_spectrum()\n",
    "l, bl = exp.beam_power_spectrum(beam_fwhm = 1.4)\n",
    "l, nl = exp.white_noise_power_spectrum(noiseval_white = 2.0)\n",
    "l, nl_deconvolved =  exp.white_noise_power_spectrum(noiseval_white = 2.0, beam_fwhm = 1.4)\n",
    "cl_noise =  nl_deconvolved\n",
    "nber_clus_fit = 30000\n",
    "nber_rand_fit = 30000\n",
    "mass_int_estimator_validation = np.arange(0, 12, 0.1)\n",
    "c200c = 3\n",
    "z = 1\n",
    "\n",
    "\n",
    "covariance_matrix_estimator_validation, correlation_matrix_estimator_validation = lensing_estimator.covariance_and_correlation_matrix(nber_cov, nber_clus, map_params, l, cl, bl = bl, nl = nl, cl_noise = cl_noise)\n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/covariance_matrix_estimator_validation.npy', 'wb') as file:\n",
    "    np.save(file, covariance_matrix_estimator_validation)   \n",
    "        \n",
    "\n",
    "model_profiles_estimator_validation = lensing_estimator.model_profiles(nber_clus_fit, nber_rand_fit, map_params, l, cl, mass_int_estimator_validation, c200c, z, bl = bl, cl_noise = cl_noise)\n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/model_profiles_estimator_validation.npy', 'wb') as file:\n",
    "    np.save(file, model_profiles_estimator_validation)    \n",
    "\n",
    "            \n",
    "            \n",
    "lk_arr_2e14 = []      \n",
    "lk_arr_6e14 = []  \n",
    "lk_arr_10e14 = []  \n",
    "for i in tqdm(range(25)):    \n",
    "    maps_clus_2e14, maps_clus_6e14, maps_clus_10e14 = sims.cmb_test_data(nber_clus, validation_analyis = True)\n",
    "    maps_rand = []\n",
    "    for j in range(nber_rand):    \n",
    "        map_rand = sims.cmb_mock_data(map_params, l, cl,  bl = bl, nl = nl)\n",
    "        maps_rand.append(map_rand)\n",
    "    bins_2e14, lensing_dipole_profile_2e14, stacks_2e14 = lensing_estimator.lensing_dipole_profile(map_params, maps_clus_2e14, maps_rand, l = l, cl = cl, cl_noise = cl_noise)\n",
    "    bins_6e14, lensing_dipole_profile_6e14, stacks_6e14 = lensing_estimator.lensing_dipole_profile(map_params, maps_clus_6e14, maps_rand, l = l, cl = cl, cl_noise = cl_noise)\n",
    "    bins_10e14, lensing_dipole_profile_10e14, stacks_10e14 = lensing_estimator.lensing_dipole_profile(map_params, maps_clus_10e14, maps_rand, l = l, cl = cl, cl_noise = cl_noise)\n",
    "    data_2e14 = bins_2e14, lensing_dipole_profile_2e14, covariance_matrix_estimator_validation\n",
    "    data_6e14 = bins_6e14, lensing_dipole_profile_6e14, covariance_matrix_estimator_validation\n",
    "    data_10e14 = bins_10e14, lensing_dipole_profile_10e14, covariance_matrix_estimator_validation\n",
    "   \n",
    "\n",
    "    likelihood_2e14, _, _ = stats.run_ml(data_2e14, model_profiles_estimator_validation, mass_int_estimator_validation)\n",
    "    mass_int_lk_2e14, lk_2e14 = likelihood_2e14\n",
    "    likelihood_6e14, _, _ = stats.run_ml(data_6e14, model_profiles_estimator_validation, mass_int_estimator_validation)\n",
    "    mass_int_lk_6e14, lk_6e14 = likelihood_6e14\n",
    "    likelihood_10e14, _, _ = stats.run_ml(data_10e14, model_profiles_estimator_validation, mass_int_estimator_validation)\n",
    "    mass_int_lk_10e14, lk_10e14 = likelihood_10e14\n",
    "    lk_arr_2e14.append(lk_2e14)\n",
    "    lk_arr_6e14.append(lk_6e14)\n",
    "    lk_arr_10e14.append(lk_10e14)\n",
    "\n",
    "    del maps_clus_2e14\n",
    "    del maps_clus_6e14\n",
    "    del maps_clus_10e14\n",
    "    del maps_rand\n",
    "    \n",
    "    \n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/lk_arr_estimator_validation_2e14.npy', 'wb') as file:\n",
    "    np.save(file, lk_arr_2e14) \n",
    "    \n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/lk_arr_estimator_validation_6e14.npy', 'wb') as file:\n",
    "    np.save(file, lk_arr_6e14) \n",
    "    \n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/lk_arr_estimator_validation_10e14.npy', 'wb') as file:\n",
    "    np.save(file, lk_arr_10e14) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/lk_arr_estimator_validation_2e14.npy', 'rb') as file:\n",
    "    lk_arr_2e14 = np.load(file) \n",
    "    \n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/lk_arr_estimator_validation_6e14.npy', 'rb') as file:\n",
    "    lk_arr_6e14 = np.load(file) \n",
    "    \n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/lk_arr_estimator_validation_10e14.npy', 'rb') as file:\n",
    "    lk_arr_10e14 = np.load(file) \n",
    "    \n",
    "    \n",
    "comb_lk_2e14 = np.ones(len(lk_arr_2e14[0]))\n",
    "for i in range(1):\n",
    "    comb_lk_2e14 *= lk_arr_2e14[i]\n",
    "comb_lk_2e14 = comb_lk_2e14/max(comb_lk_2e14)\n",
    "median_value, error = stats.ml_params(mass_int_lk_2e14, comb_lk_2e14)      \n",
    "print(median_value, error) \n",
    "\n",
    "comb_lk_6e14 = np.ones(len(lk_arr_6e14[0]))\n",
    "for i in range(1):\n",
    "    comb_lk_6e14 *= lk_arr_6e14[i]\n",
    "comb_lk_6e14 = comb_lk_6e14/max(comb_lk_6e14)\n",
    "median_value, error = stats.ml_params(mass_int_lk_6e14, comb_lk_6e14)      \n",
    "print(median_value, error) \n",
    "      \n",
    "comb_lk_10e14 = np.ones(len(lk_arr_10e14[0]))\n",
    "for i in range(1):\n",
    "    comb_lk_10e14 *= lk_arr_10e14[i]\n",
    "comb_lk_10e14 = comb_lk_10e14/max(comb_lk_10e14)\n",
    "median_value, error = stats.ml_params(mass_int_lk_10e14, comb_lk_10e14)      \n",
    "print(median_value, error)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, sharex='col', sharey='row',\n",
    "                        gridspec_kw={'hspace': 0, 'wspace': 0}, figsize=(16,5))\n",
    "(ax1, ax2, ax3) = axs\n",
    "for i in range(2):\n",
    "    ax1.plot(mass_int_lk_2e14, lk_arr_2e14[i], color = 'red', ls='-', alpha = 0.1)\n",
    "ax1.plot(mass_int_lk_2e14, comb_lk_2e14, color = 'black', ls = '-', label = 'Combined')\n",
    "ax1.tick_params(labelsize = sz_lb)\n",
    "ax1.axvline(2, color = 'green', ls = '--', label = 'True') \n",
    "ax1.set_xlabel(r'$M_{200}\\ [10^{14}M_\\odot]$', fontsize = sz_ft)   \n",
    "ax1.set_ylabel('Normalized '+r'$\\mathcal{L}$', fontsize = sz_ft)\n",
    "ax1.legend(prop={'size': sz_lb}) \n",
    "for i in range(2):\n",
    "    ax2.plot(mass_int_lk_6e14[4500:7500], lk_arr_6e14[i][4500:7500], color = 'red', ls='-', alpha = 0.1)\n",
    "ax2.plot(mass_int_lk_6e14[4500:7500], comb_lk_6e14[4500:7500], color = 'black', ls = '-', label = 'Combined')\n",
    "ax2.tick_params(labelsize = sz_lb)\n",
    "ax2.axvline(6, color = 'green', ls = '--', label = 'True') \n",
    "ax2.set_xlabel(r'$M_{200}\\ [10^{14}M_\\odot]$', fontsize = sz_ft) \n",
    "ax2.legend(prop={'size': sz_lb}) \n",
    "for i in range(2):\n",
    "    ax3.plot(mass_int_lk_10e14[8000:12000], lk_arr_10e14[i][8000:12000], color = 'red', ls='-', alpha = 0.1)\n",
    "ax3.plot(mass_int_lk_10e14[8000:12000], comb_lk_10e14[8000:12000], color = 'black', ls = '-', label = 'Combined')\n",
    "ax3.tick_params(labelsize = sz_lb)\n",
    "ax3.axvline(10, color = 'green', ls = '--', label = 'True') \n",
    "ax3.set_xlabel(r'$M_{200}\\ [10^{14}M_\\odot]$', fontsize = sz_ft) \n",
    "ax3.legend(prop={'size': sz_lb}) \n",
    "\n",
    "plt.savefig('/Volumes/Extreme_SSD/codes/master_thesis/code/figures/estimator_validation.eps', dpi = 200., bbox_inches = 'tight', pad_inches = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-belize",
   "metadata": {},
   "source": [
    "## Estimator Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "nber_clus = 3000\n",
    "nber_rand = 30000\n",
    "nber_cov = 500\n",
    "map_params = [240, 0.25, 240, 0.25]\n",
    "l, cl = CosmoCalc().cmb_power_spectrum()\n",
    "cluster = [2e14, 3, 0.7]\n",
    "l, bl = exp.beam_power_spectrum(beam_fwhm = 1.0)\n",
    "l, nl1 = exp.white_noise_power_spectrum(noiseval_white = 0.1)\n",
    "l, nl2 = exp.white_noise_power_spectrum(noiseval_white = 0.5)\n",
    "l, nl3 = exp.white_noise_power_spectrum(noiseval_white = 1.0)\n",
    "l, nl4 = exp.white_noise_power_spectrum(noiseval_white = 3.0)\n",
    "l, nl_deconvolved1 =  exp.white_noise_power_spectrum(noiseval_white = 0.1, beam_fwhm = 1.0)\n",
    "l, nl_deconvolved2 =  exp.white_noise_power_spectrum(noiseval_white = 0.1, beam_fwhm = 1.0)\n",
    "l, nl_deconvolved3 =  exp.white_noise_power_spectrum(noiseval_white = 0.1, beam_fwhm = 1.0)\n",
    "l, nl_deconvolved4 =  exp.white_noise_power_spectrum(noiseval_white = 0.1, beam_fwhm = 1.0)\n",
    "cl_noise1 =  nl_deconvolved1\n",
    "cl_noise2 =  nl_deconvolved2\n",
    "cl_noise3 =  nl_deconvolved3\n",
    "cl_noise4 =  nl_deconvolved4\n",
    "nber_clus_fit = 30000\n",
    "nber_rand_fit = 30000\n",
    "mass_int_estimator_comparison = np.arange(0, 4, 0.1)\n",
    "c200c = 3\n",
    "z = 0.7\n",
    "\n",
    "\n",
    "covariance_matrix_estimator_comparison1, correlation_matrix_estimator_comparison1 = lensing_estimator.covariance_and_correlation_matrix(nber_cov, nber_clus, map_params, l, cl, bl = bl, nl = nl1, cl_noise = cl_noise1)\n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/covariance_matrix_estimator_comparison_0.1.npy', 'wb') as file:\n",
    "    np.save(file, covariance_matrix_estimator_comparison1)  \n",
    "        \n",
    "covariance_matrix_estimator_comparison2, correlation_matrix_estimator_comparison2 = lensing_estimator.covariance_and_correlation_matrix(nber_cov, nber_clus, map_params, l, cl, bl = bl, nl = nl2, cl_noise = cl_noise2)\n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/covariance_matrix_estimator_comparison_0.5.npy', 'wb') as file:\n",
    "    np.save(file, covariance_matrix_estimator_comparison2)  \n",
    "    \n",
    "covariance_matrix_estimator_comparison3, correlation_matrix_estimator_comparison3 = lensing_estimator.covariance_and_correlation_matrix(nber_cov, nber_clus, map_params, l, cl, bl = bl, nl = nl3, cl_noise = cl_noise3)\n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/covariance_matrix_estimator_comparison_1.0.npy', 'wb') as file:\n",
    "    np.save(file, covariance_matrix_estimator_comparison3)  \n",
    "\n",
    "covariance_matrix_estimator_comparison4, correlation_matrix_estimator_comparison4 = lensing_estimator.covariance_and_correlation_matrix(nber_cov, nber_clus, map_params, l, cl, bl = bl, nl = nl4, cl_noise = cl_noise4)\n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/covariance_matrix_estimator_comparison_3.0.npy', 'wb') as file:\n",
    "    np.save(file, covariance_matrix_estimator_comparison4)  \n",
    "\n",
    "\n",
    "model_profiles_estimator_comparison1 = lensing_estimator.model_profiles(nber_clus_fit, nber_rand_fit, map_params, l, cl, mass_int_estimator_comparison, c200c, z, bl = bl, cl_noise = cl_noise1)\n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/model_profiles_estimator_comparison_0.1.npy', 'wb') as file:\n",
    "    np.save(file, model_profiles_estimator_comaprison1)    \n",
    "\n",
    "model_profiles_estimator_comparison2 = lensing_estimator.model_profiles(nber_clus_fit, nber_rand_fit, map_params, l, cl, mass_int_estimator_comparison, c200c, z, bl = bl, cl_noise = cl_noise2)\n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/model_profiles_estimator_comparison_0.5.npy', 'wb') as file:\n",
    "    np.save(file, model_profiles_estimator_comaprison2)   \n",
    "    \n",
    "model_profiles_estimator_comparison3 = lensing_estimator.model_profiles(nber_clus_fit, nber_rand_fit, map_params, l, cl, mass_int_estimator_comparison, c200c, z, bl = bl, cl_noise = cl_noise3)\n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/model_profiles_estimator_comparison_1.0.npy', 'wb') as file:\n",
    "    np.save(file, model_profiles_estimator_comaprison3)   \n",
    "    \n",
    "model_profiles_estimator_comparison4 = lensing_estimator.model_profiles(nber_clus_fit, nber_rand_fit, map_params, l, cl, mass_int_estimator_comparison, c200c, z, bl = bl, cl_noise = cl_noise4)\n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/model_profiles_estimator_comparison_3.0.npy', 'wb') as file:\n",
    "    np.save(file, model_profiles_estimator_comaprison4)   \n",
    "       \n",
    "\n",
    "\n",
    "for i in tqdm(range(1)):    \n",
    "    maps_clus1 = []\n",
    "    for j in range(nber_clus):    \n",
    "        map_clus = sims.cmb_mock_data(map_params, l, cl, cluster = cluster bl = bl, nl = nl1)\n",
    "        maps_clus1.append(map_clus)    \n",
    "    maps_rand1 = []\n",
    "    for j in range(nber_rand):    \n",
    "        map_rand = sims.cmb_mock_data(map_params, l, cl,  bl = bl, nl = nl1)\n",
    "        maps_rand1.append(map_rand)\n",
    "    bins1, lensing_dipole_profile1, stacks1 = lensing_estimator.lensing_dipole_profile(map_params, maps_clus1, maps_rand1,  l = l, cl = cl, cl_noise = cl_noise1)\n",
    "    data1 = bins1, lensing_dipole_profile1, covariance_matrix_estimator_comparison1\n",
    "    likelihood1, median_mass1, error1 = stats.run_ml(data1, model_profiles_estimator_comparison1, mass_int_estimator_comparison)\n",
    "    del maps_clus1\n",
    "    del maps_rand1\n",
    "    \n",
    "    maps_clus2 = []\n",
    "    for j in range(nber_clus):    \n",
    "        map_clus = sims.cmb_mock_data(map_params, l, cl, cluster = cluster bl = bl, nl = nl2)\n",
    "        maps_clus2.append(map_clus)    \n",
    "    maps_rand2 = []\n",
    "    for j in range(nber_rand):    \n",
    "        map_rand = sims.cmb_mock_data(map_params, l, cl,  bl = bl, nl = nl2)\n",
    "        maps_rand2.append(map_rand)\n",
    "    bins2, lensing_dipole_profile2, stacks2 = lensing_estimator.lensing_dipole_profile(map_params, maps_clus2, maps_rand2,  l = l, cl = cl, cl_noise = cl_noise2)\n",
    "    data2 = bins2, lensing_dipole_profile2, covariance_matrix_estimator_comparison2\n",
    "    likelihood2, median_mass2, error2 = stats.run_ml(data2, model_profiles_estimator_comparison2, mass_int_estimator_comparison)\n",
    "    del maps_clus2\n",
    "    del maps_rand2  \n",
    "  \n",
    "    maps_clus3 = []\n",
    "    for j in range(nber_clus):    \n",
    "        map_clus = sims.cmb_mock_data(map_params, l, cl, cluster = cluster bl = bl, nl = nl3)\n",
    "        maps_clus3.append(map_clus)    \n",
    "    maps_rand3 = []\n",
    "    for j in range(nber_rand):    \n",
    "        map_rand = sims.cmb_mock_data(map_params, l, cl,  bl = bl, nl = nl3)\n",
    "        maps_rand3.append(map_rand)\n",
    "    bins3, lensing_dipole_profile3, stacks3 = lensing_estimator.lensing_dipole_profile(map_params, maps_clus3, maps_rand3,  l = l, cl = cl, cl_noise = cl_noise3)\n",
    "    data3 = bins3, lensing_dipole_profile3, covariance_matrix_estimator_comparison3\n",
    "    likelihood3, median_mass3, error3 = stats.run_ml(data3, model_profiles_estimator_comparison3, mass_int_estimator_comparison)\n",
    "    del maps_clus3\n",
    "    del maps_rand3\n",
    "    \n",
    "    maps_clus4 = []\n",
    "    for j in range(nber_clus):    \n",
    "        map_clus = sims.cmb_mock_data(map_params, l, cl, cluster = cluster bl = bl, nl = nl4)\n",
    "        maps_clus4.append(map_clus)    \n",
    "    maps_rand4 = []\n",
    "    for j in range(nber_rand):    \n",
    "        map_rand = sims.cmb_mock_data(map_params, l, cl,  bl = bl, nl = nl4)\n",
    "        maps_rand4.append(map_rand)\n",
    "    bins4, lensing_dipole_profile4, stacks4 = lensing_estimator.lensing_dipole_profile(map_params, maps_clus4, maps_rand4,  l = l, cl = cl, cl_noise = cl_noise4)\n",
    "    data4 = bins4, lensing_dipole_profile4, covariance_matrix_estimator_comparison4\n",
    "    likelihood4, median_mass4, error4 = stats.run_ml(data4, model_profiles_estimator_comparison4, mass_int_estimator_comparison)\n",
    "    del maps_clus4\n",
    "    del maps_rand4\n",
    "    \n",
    "    \n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/mass_result_estimator_comparison_0.1.npy', 'wb') as file:\n",
    "    np.save(file, median_mass1)\n",
    "    np.save(file, error1)\n",
    "    \n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/mass_result_estimator_comparison_0.5.npy', 'wb') as file:\n",
    "    np.save(file, median_mass2)\n",
    "    np.save(file, error2)\n",
    "    \n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/mass_result_estimator_comparison_1.0.npy', 'wb') as file:\n",
    "    np.save(file, median_mass3)\n",
    "    np.save(file, error3) \n",
    "    \n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/mass_result_estimator_comparison_3.0.npy', 'wb') as file:\n",
    "    np.save(file, median_mass4)\n",
    "    np.save(file, error4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/mass_result_estimator_comparison_0.1.npy', 'rb') as file:\n",
    "    median_mass1 = np.load(file)\n",
    "    error1 = np.load(file)\n",
    "    \n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/mass_result_estimator_comparison_0.5.npy', 'rb') as file:\n",
    "    median_mass2 = np.load(file)\n",
    "    error2 = np.load(file)\n",
    "    \n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/mass_result_estimator_comparison_1.0.npy', 'rb') as file:\n",
    "    median_mass3 = np.load(file)\n",
    "    error3 = np.load(file)\n",
    "    \n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/mass_result_estimator_comparison_3.0.npy', 'rb') as file:\n",
    "    median_mass4 = np.load(file)\n",
    "    error4 = np.load(file)\n",
    "    \n",
    "noise_arr = [0.1, 0.5, 1.0, 3.0]\n",
    "frac_mass_unc = [error1/median_mass1, error2/median_mass2, error3/median_mass3, error4/median_mass4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( figsize=(9,9))\n",
    "\n",
    "color_arr = ['indigo', 'royalblue', 'lightseagreen', 'darkgreen']\n",
    "\n",
    "ax.plot(noise_arr, frac_mass_unc,  color = color_arr[0], label = 'This Work')\n",
    "ax.set_xlabel(r'$\\Delta T\\ [\\mu$'+'K]$', fontsize = sz_ft)\n",
    "ax.set_ylabel(r'$\\Delta M/M$', fontsize = sz_ft\n",
    "ax.tick_params(labelsize = sz_lb)\n",
    "ax.legend(prop={'size': sz_lb}) \n",
    "\n",
    "fig.savefig('/Volumes/Extreme_SSD/codes/master_thesis/code/figures/estimator_comparison.eps', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-elimination",
   "metadata": {},
   "source": [
    "## Cluster Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "nber_clus = 3000\n",
    "nber_rand = 30000\n",
    "nber_cov = 500\n",
    "map_params = [240, 0.25, 240, 0.25]\n",
    "l, cl = CosmoCalc().cmb_power_spectrum()\n",
    "l, bl = exp.beam_power_spectrum(beam_fwhm = 1.4)\n",
    "l, nl = exp.white_noise_power_spectrum(noiseval_white = 2.0)\n",
    "l, nl_deconvolved =  exp.white_noise_power_spectrum(noiseval_white = 2.0, beam_fwhm = 1.4)\n",
    "cl_noise =  nl_deconvolved\n",
    "nber_clus_fit = 30000\n",
    "nber_rand_fit = 30000\n",
    "centroid_shift_value = 0.5\n",
    "mass_int_baseline = np.arange(0, 12, 0.1)\n",
    "mass_int_centroid_shift = np.arange(4, 8, 0.1) \n",
    "c200c = 3\n",
    "z = 1\n",
    "\n",
    "\n",
    "\n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/covariance_matrix_estimator_validation.npy', 'rb') as file:\n",
    "    covariance_matrix = np.load(file) \n",
    "        \n",
    "\n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/model_profiles_estimator_validation.npy', 'rb') as file:\n",
    "    model_profiles_baseline = np.load(file)          \n",
    "        \n",
    "model_profiles_centroid_shift = lensing_estimator.model_profiles(nber_clus_fit, nber_rand_fit, map_params, l, cl, mass_int_estimator_validation, c200c, z, centroid_shift_value = centroid_shift_value, bl = bl, cl_noise = cl_noise)\n",
    "with open('/Volumes/Extreme_SSD/codes/master_thesis/code/results/model_profiles_centroid_shift.npy', 'wb') as file:\n",
    "    np.save(file, model_profiles_centroid_shift)     \n",
    "\n",
    "    \n",
    "    \n",
    "lk_arr_no_shift, mass_arr_no_shift, error_arr_no_shift = [], [], []    \n",
    "lk_arr_ignoring_shift, mass_arr_ignoring_shift, error_arr_ignoring_shift = [], [], []\n",
    "lk_arr_centroid_shift, mass_arr_centroid_shift, error_arr_centroid_shift = [], [], []   \n",
    "for i in tqdm(range(25)):    \n",
    "    maps_clus_no_shift, maps_clus_centroid_shift = sims.cmb_test_data(nber_clus, clus_position_analysis = True)\n",
    "    maps_rand = []\n",
    "    for j in range(nber_rand):    \n",
    "        map_rand = sims.cmb_mock_data(map_params, l, cl,  bl = bl, nl = nl)\n",
    "        maps_rand.append(map_rand)\n",
    "                                                                               \n",
    "    bins_no_shift, lensing_dipole_profile_no_shift, stacks_no_shift = lensing_estimator.lensing_dipole_profile(map_params, maps_clus_no_shift, maps_rand,  l = l, cl = cl, cl_noise = cl_noise)\n",
    "    bins_centroid_shift, lensing_dipole_profile_centroid_shift, stacks_centroid_shift = lensing_estimator.lensing_dipole_profile(map_params, maps_clus_centroid_shift, maps_rand, l = l, cl = cl, cl_noise = cl_noise)\n",
    "    data_no_shift = bins_no_shift, lensing_dipole_profile_no_shift, covariance_matrix\n",
    "    data_centroid_shift = bins_centroid_shift, lensing_dipole_profile_centroid_shift, covariance_matrix\n",
    "\n",
    "    likelihood_no_shift, median_mass_no_shift, error_no_shift = stats.run_ml(data_no_shift, model_profiles_baseline, mass_int_baseline)\n",
    "    likelihood_ignoring_shift, median_mass_ignoring_shift, error_ignoring_shift = stats.run_ml(data_centroid_shift, model_profiles_baseline, mass_int_baseline)\n",
    "    likelihood_centroid_shift, medan_mass_centroid_shift, error_centroid_shift = stats.run_ml(data_centroid_shift, model_profiles_centroid_shift, mass_int_centroid_shift)\n",
    "    mass_int_lk_no_shift, lk_no_shift = likelihood_no_shift\n",
    "    mass_int_lk_ignoring_shift, lk_ignoring_shift = likelihood_ignoring_shift\n",
    "    mass_int_lk_centroid_shift, lk_centroid_shift = likelihood_centroid_shift\n",
    "    lk_arr_no_shift.append(lk_no_shift)\n",
    "    lk_arr_ignoring_shift.append(lk_ignoring_shift)\n",
    "    lk_arr_centroid_shift.append(lk_centroid_shift)\n",
    "    mass_arr_no_shift.append(median_mass_no_shift)\n",
    "    mass_arr_ignoring_shift.append(median_mass_ignoring_shift)\n",
    "    mass_arr_centroid_shift.append(median_mass_centroid_shift)\n",
    "    error_arr_no_shift.append(error_no_shift)\n",
    "    error_arr_ignoring_shift.append(errors_ignoring_shift)\n",
    "    error_arr_centroid_shift.append(errors_centroid_shift)\n",
    "   \n",
    "    \n",
    "    \n",
    "    del maps_clus_no_shift\n",
    "    del maps_clus_centroid_shift\n",
    "    del maps_rand\n",
    "\n",
    "    \n",
    "with open('mass_arr_no_shift.npy', 'wb') as file:\n",
    "    np.save(file, mass_arr_no_shift) \n",
    "    \n",
    "with open('mass_arr_ignoring_shift.npy', 'wb') as file:\n",
    "    np.save(file, mass_arr_ignoring_shift) \n",
    "    \n",
    "with open('mass_arr_centroid_shift.npy', 'wb') as file:\n",
    "    np.save(file, mass_arr_centroid_shift) \n",
    "\n",
    "with open('error_arr_no_shift.npy', 'wb') as file:\n",
    "    np.save(file, error_arr_no_shift) \n",
    "    \n",
    "with open('error_arr_ignoring_shift.npy', 'wb') as file:\n",
    "    np.save(file, error_arr_ignoring_shift) \n",
    "    \n",
    "with open('mass_arr_centroid_shift.npy', 'wb') as file:\n",
    "    np.save(file, error_arr_centroid_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mass_arr_no_shift.npy', 'rb') as file:\n",
    "    mass_arr_no_shift = np.load(file) \n",
    "    \n",
    "with open('mass_arr_ignoring_shift.npy', 'rb') as file:\n",
    "    mass_arr_ignoring_shift = np.load(file) \n",
    "    \n",
    "with open('mass_arr_centroid_shift.npy', 'rb') as file:\n",
    "    mass_arr_centroid_shift = np.load(file) \n",
    "\n",
    "with open('error_arr_no_shift.npy', 'rb') as file:\n",
    "    error_arr_ignoring_shift = np.load(file) \n",
    "    \n",
    "with open('error_arr_ignoring_shift.npy', 'rb') as file:\n",
    "    error_arr_ignoring_shift = np.load(file) \n",
    "    \n",
    "with open('mass_arr_centroid_shift.npy', 'rb') as file:\n",
    "    error_arr_centroid_shift = np.load(file)\n",
    "    \n",
    "    \n",
    "comb_lk_no_shift = np.ones(len(lk_arr_no_shift[0]))\n",
    "for i in range(25):\n",
    "    comb_lk_no_shift *= lk_arr_no_shift[i]\n",
    "comb_lk_no_shift = comb_lk_no_shift/max(comb_lk_no_shift)\n",
    "mean_mass_comb_no_shift, error_comb_no_shift = stats.ml_params(mass_int_lk_no_shift, comb_lk_no_shift)      \n",
    "print(mean_mass_comb_no_shift, error_comb_no_shift)     \n",
    "    \n",
    "\n",
    "comb_lk_ignoring_shift = np.ones(len(lk_arr_ignoring_shift[0]))\n",
    "for i in range(25):\n",
    "    comb_lk_ignoring_shift *= lk_arr_ignoring_shift[i]\n",
    "comb_lk_ignoring_shift = comb_lk_ignoring_shift/max(comb_lk_ignoring_shift)\n",
    "mean_mass_comb_ignoring_shift, error_comb_ignoring_shift = stats.ml_params(mass_int_lk_ignoring_shift, comb_lk_ignoring_shift)      \n",
    "print(mean_mass_comb_ignoring, errors_comb_ignoring_shift) \n",
    "\n",
    "\n",
    "comb_lk_centroid_shift = np.ones(len(lk_arr_centroid_shift[0]))\n",
    "for i in range(25):\n",
    "    comb_lk_centroid_shift *= lk_arr_centroid_shift[i]\n",
    "comb_lk_centroid_shift = comb_lk_centroid_shift/max(comb_lk_centroid_shift)\n",
    "mean_mass_comb_centroid_shift, error_comb_centroid_shift = stats.ml_params(mass_int_lk_centroid_shift, comb_lk_centroid_shift)      \n",
    "print(mean_mass_comb_centroid_shift, errors_comb_centroid_shift) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "best_fit_arr = [mass_arr_no_shift, mass_arr_ignoring, mass_arr_centroid_shift]\n",
    "centers_no_shift = np.asarray(mass_arr_no_shift)-6\n",
    "centers_ignoring = np.asarray(mass_arr_ignoring)-6\n",
    "centers_centroid_shift = np.asarray(mass_arr_centroid_shift)-6\n",
    "centers = [centers_no_shift, centers_ignoring, centers_centroid_shift]\n",
    "width_arr = [error_arr_no_shift, error_arr_ignoring, error_arr_centroid_shift]\n",
    "color_arr = ['black', 'red', 'blue']\n",
    "for cntr, (center, widthval, colorval) in enumerate(zip(centers, width_arr, color_arr )):\n",
    "    yval = [cntr for i in range(len(mass_arr_no_shift))]\n",
    "    plt.errorbar(center, yval, xerr = widthval, color = colorval, marker = 'o', ls = '', alpha = 0.05)\n",
    "plt.errorbar(mean_mass_comb_no_shift-6, 0, xerr = (errors_comb_no_shift[0]+errors_comb_no_shift[1])/2, color = 'black', marker = 'o', ls = '')\n",
    "plt.errorbar(mean_mass_comb_ignoring-6, 1, xerr = (errors_comb_ignoring[0]+errors_ignoring[1])/2, color = 'red', marker = 'o', ls = '')\n",
    "plt.errorbar(mean_mass_comb_centroid_shift-6, 2, xerr = (errors_comb_centroid_shift[0]+errors_centroid_shift[1])/2, color = 'blue', marker = 'o', ls = '')\n",
    "plt.axvline(0, color = 'green', ls = '--')\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.axes.yaxis.set_ticks([])\n",
    "plt.text(0.6, 0, 'baseline', fontsize = 15)\n",
    "plt.text(0.6, 1, '+ 0.5\\' shift', fontsize = 15)\n",
    "plt.text(0.6, 2, 'corrected', fontsize = 15)\n",
    "plt.xlabel(r'$M_{lens}-M_{true}$', fontsize = 15)\n",
    "plt.savefig('/Volumes/Extreme_SSD/codes/master_thesis/code/figures/cluster_positions.eps', dpi = 200., bbox_inches = 'tight', pad_inches = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-chemical",
   "metadata": {},
   "source": [
    "## Foreground Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "nber_clus = 3000\n",
    "nber_rand = 30000\n",
    "nber_cov = 250\n",
    "map_params = [240, 0.25, 240, 0.25]\n",
    "l, cl = CosmoCalc().cmb_power_spectrum()\n",
    "c500 = concentration.concentration(2e14, '500c', 0.7)\n",
    "M200c, _, c200 = mass_defs.changeMassDefinition(2e14, c500, 0.7, '500c', '200c', profile='nfw')\n",
    "fname = 'sim_data/mdpl2_cutouts_for_tszksz_clus_detection_M1.7e+14to2.3e+14_z0.6to0.8_15320haloes_boxsize20.0am.npz'\n",
    "cutouts_dic = np.load(fname, allow_pickle = 1, encoding= 'latin1')['arr_0'].item()\n",
    "mass_z_key = list(cutouts_dic.keys())[0]\n",
    "cutouts = cutouts_dic[mass_z_key]\n",
    "scale_fac = fg.compton_y_to_delta_Tcmb(freq = 150, uK = True)\n",
    "tsz_cutouts, ksz_cutouts, tsz_ksz_cutouts = [], [], []\n",
    "for kcntr, keyname in enumerate( cutouts ):\n",
    "    tsz_cutout = cutouts[keyname]['y']*scale_fac\n",
    "    tsz_cutouts.append(tsz_cutout)\n",
    "    ksz_cutout = cutouts[keyname]['ksz']*random.randrange(-1, 2, 2)\n",
    "    ksz_cutouts.append(ksz_cutout)\n",
    "    tsz_ksz_cutout = tsz_cutout + ksz_cutout\n",
    "    tsz_ksz_cutouts.append(tsz_ksz_cutout)   \n",
    "l, bl = exp.beam_power_spectrum(beam_fwhm = 1.4)\n",
    "l, nl = exp.noise_power_spectrum(noiseval_white = 2.0)\n",
    "l, nl_deconvolved =  exp.noise_power_spectrum(noiseval_white = 2.0, beam_fwhm = 1.4)\n",
    "cl_noise =  nl_deconvolved\n",
    "nber_clus_fit = 30000 \n",
    "nber_rand_fit = 30000\n",
    "mass_int_bias = np.arange(0, 6, 0.2) \n",
    "           \n",
    "\n",
    "#covariance_matrix, correlation_matrix = lensing_estimator.covariance_matrix(nber_clus, nber_rand, nber_cov, map_params,  l, cl, bl = bl, nl = nl, cl_noise = cl_noise)\n",
    "with open('covariance_matrix.npy', 'rb') as file:\n",
    "    covariance_matrix = np.load(file)   \n",
    "        \n",
    "\n",
    "        \n",
    "#covariance_matrix_tsz, correlation_matrix_tsz = lensing_estimator.covariance_matrix(nber_clus, nber_rand, nber_cov, map_params, l, cl, tsz_cutouts, bl = bl, nl = nl, cl_noise = cl_noise)      \n",
    "with open('covariance_matrix_tsz.npy', 'rb') as file:\n",
    "    covariance_matrix_tsz = np.load(file)\n",
    "    \n",
    "    \n",
    "#covariance_matrix_ksz, correlation_matrix_ksz = lensing_estimator.covariance_matrix(nber_clus, nber_rand, nber_cov, map_params, l, cl, ksz_cutouts, bl = bl, nl = nl, cl_noise = cl_noise)      \n",
    "with open('covariance_matrix_ksz.npy', 'rb') as file:\n",
    "    covariance_matrix_ksz = np.load(file)    \n",
    "    \n",
    "    \n",
    "#covariance_matrix_tsz_ksz, correlation_matrix_tsz_ksz = lensing_estimator.covariance_matrix(nber_clus, nber_rand, nber_cov, map_params, l, cl, tsz_ksz_cutouts, bl = bl, nl = nl, cl_noise = cl_noise)      \n",
    "with open('covariance_matrix_tsz_ksz.npy', 'rb') as file:\n",
    "    covariance_matrix_tsz_ksz = np.load(file)       \n",
    "    \n",
    "\n",
    "#covariance_matrix_tsz_corrected, correlation_matrix_tsz_corrected = lensing_estimator.covariance_matrix(nber_clus, nber_rand, nber_cov, map_params, l, cl, tsz_cutouts, bl = bl, nl = nl, cl_noise = cl_noise, correct_for_tsz = True)      \n",
    "with open('covariance_matrix_tsz_corrected.npy', 'rb') as file:\n",
    "    covariance_matrix_tsz_corrected = np.load(file)\n",
    "    \n",
    "    \n",
    "#covariance_matrix_tsz_ksz_corrected, correlation_matrix_tsz_ksz_corrected = lensing_estimator.covariance_matrix(nber_clus, nber_rand, nber_cov, map_params, l, cl, tsz_ksz_cutouts, bl = bl, nl = nl, cl_noise = cl_noise, correct_for_tsz = True)      \n",
    "with open('covariance_matrix_tsz_ksz_corrected.npy', 'rb') as file:\n",
    "    covariance_matrix_tsz_ksz_corrected = np.load(file)\n",
    "  \n",
    "        \n",
    "#dipole_profile_models_bias = lensing_estimator.fit_profiles(nber_clus_fit, nber_rand_fit, map_params, l, cl, mass_int_bias*1e14, c200, 0.7, bl = bl, nl =nl, cl_noise = cl_noise)\n",
    "with open('dipole_profile_models_foregrounds.npy', 'rb') as file:\n",
    "    dipole_profile_models_bias = np.load(file)\n",
    "\n",
    "    \n",
    "lk_arr_baseline, mass_arr_baseline, error_arr_baseline = [], [], []      \n",
    "lk_arr_tsz, mass_arr_tsz, error_arr_tsz = [], [], []      \n",
    "lk_arr_ksz, mass_arr_ksz, error_arr_ksz = [], [], []  \n",
    "lk_arr_tsz_ksz, mass_arr_tsz_ksz, error_arr_tsz_ksz = [], [], []\n",
    "lk_arr_tsz_corrected, mass_arr_tsz_corrected, error_arr_tsz_corrected = [], [], []\n",
    "lk_arr_tsz_ksz_corrected, mass_arr_tsz_ksz_corrected, error_arr_tsz_ksz_corrected = [], [], []\n",
    "for i in tqdm(range(25)):    \n",
    "    maps_baseline, maps_tsz, maps_ksz, maps_tsz_ksz = sims.cmb_test_data(nber_clus, extragal_bias_analysis = True)\n",
    "    maps_rand = sims.cmb_mock_data(nber_rand, map_params, l, cl,  bl = bl, nl = nl)\n",
    "    bins_baseline, dipole_profile_baseline, stacks_baseline = lensing_estimator.get_dipole_profile(map_params, maps_baseline, maps_rand,  l, cl, cl_noise)\n",
    "    bins_tsz, dipole_profile_tsz, stacks_tsz = lensing_estimator.get_dipole_profile(map_params, maps_tsz, maps_rand,  l, cl, cl_noise)\n",
    "    bins_ksz, dipole_profile_ksz, stacks_ksz = lensing_estimator.get_dipole_profile(map_params, maps_ksz, maps_rand,  l, cl, cl_noise)\n",
    "    bins_tsz_ksz, dipole_profile_tsz_ksz, stacks_tsz_ksz = lensing_estimator.get_dipole_profile(map_params, maps_tsz_ksz, maps_rand,  l, cl, cl_noise)\n",
    "    bins_tsz_corrected, dipole_profile_tsz_corrected, stacks_tsz_corrected = lensing_estimator.get_dipole_profile(map_params, maps_tsz, maps_rand,  l, cl, cl_noise, correct_for_tsz = True)\n",
    "    bins_tsz_ksz_corrected, dipole_profile_tsz_ksz_corrected, stacks_tsz_ksz_corrected = lensing_estimator.get_dipole_profile(map_params, maps_tsz_ksz, maps_rand, l, cl, cl_noise, correct_for_tsz = True)\n",
    "    data_baseline = bins_baseline, dipole_profile_baseline, covariance_matrix\n",
    "    data_tsz = bins_tsz, dipole_profile_tsz, covariance_matrix_tsz\n",
    "    data_ksz = bins_ksz, dipole_profile_ksz, covariance_matrix_ksz\n",
    "    data_tsz_ksz = bins_tsz_ksz, dipole_profile_tsz_ksz, covariance_matrix_tsz_ksz\n",
    "    data_tsz_corrected = bins_tsz_corrected, dipole_profile_tsz_corrected, covariance_matrix_tsz_corrected\n",
    "    data_tsz_ksz_corrected = bins_tsz_ksz_corrected, dipole_profile_tsz_ksz_corrected, covariance_matrix_tsz_ksz_corrected\n",
    "    \n",
    "   \n",
    "    likelihood_baseline, mean_mass_baseline, errors_baseline = stats.run_ml(data_baseline, dipole_profile_models_bias, mass_int_bias)\n",
    "    mass_int_lk_baseline, lk_baseline = likelihood_baseline\n",
    "    likelihood_tsz, mean_mass_tsz, errors_tsz = stats.run_ml(data_tsz, dipole_profile_models_bias, mass_int_bias)\n",
    "    mass_int_lk_tsz, lk_tsz = likelihood_tsz\n",
    "    likelihood_ksz, mean_mass_ksz, errors_ksz = stats.run_ml(data_ksz, dipole_profile_models_bias, mass_int_bias)\n",
    "    mass_int_lk_ksz, lk_ksz = likelihood_ksz\n",
    "    likelihood_tsz_ksz, mean_mass_tsz_ksz, errors_tsz_ksz = stats.run_ml(data_tsz_ksz, dipole_profile_models_bias, mass_int_bias)\n",
    "    mass_int_lk_tsz_ksz, lk_tsz_ksz = likelihood_tsz_ksz\n",
    "    likelihood_tsz_corrected, mean_mass_tsz_corrected, errors_tsz_corrected = stats.run_ml(data_tsz_corrected, dipole_profile_models_bias, mass_int_bias)\n",
    "    mass_int_lk_tsz_corrected, lk_tsz_corrected = likelihood_tsz_corrected\n",
    "    likelihood_tsz_ksz_corrected, mean_mass_tsz_ksz_corrected, errors_tsz_ksz_corrected = stats.run_ml(data_tsz_ksz_corrected, dipole_profile_models_bias, mass_int_bias)\n",
    "    mass_int_lk_tsz_ksz_corrected, lk_tsz_ksz_corrected = likelihood_tsz_ksz_corrected\n",
    "    lk_arr_baseline.append(lk_baseline)\n",
    "    lk_arr_tsz.append(lk_tsz)\n",
    "    lk_arr_ksz.append(lk_ksz)\n",
    "    lk_arr_tsz_ksz.append(lk_tsz_ksz)\n",
    "    lk_arr_tsz_corrected.append(lk_tsz_corrected)\n",
    "    lk_arr_tsz_ksz_corrected.append(lk_tsz_ksz_corrected)\n",
    "    mass_arr_baseline.append(mean_mass_baseline)\n",
    "    mass_arr_tsz.append(mean_mass_tsz)\n",
    "    mass_arr_ksz.append(mean_mass_ksz)\n",
    "    mass_arr_tsz_ksz.append(mean_mass_tsz_ksz)\n",
    "    mass_arr_tsz_corrected.append(mean_mass_tsz_corrected)\n",
    "    mass_arr_tsz_ksz_corrected.append(mean_mass_tsz_ksz_corrected)   \n",
    "    error_arr_baseline.append((errors_baseline[0]+errors_baseline[1])/2)\n",
    "    error_arr_tsz.append((errors_tsz[0]+errors_tsz[1])/2)\n",
    "    error_arr_ksz.append((errors_ksz[0]+errors_ksz[1])/2)\n",
    "    error_arr_tsz_ksz.append((errors_tsz_ksz[0]+errors_tsz_ksz[1])/2)\n",
    "    error_arr_tsz_corrected.append((errors_tsz_corrected[0]+errors_tsz_corrected[1])/2)\n",
    "    error_arr_tsz_ksz_corrected.append((errors_tsz_ksz_corrected[0]+errors_tsz_ksz_corrected[1])/2)  \n",
    "    \n",
    "    del maps_rand \n",
    "    del maps_baseline\n",
    "    del maps_tsz\n",
    "    del maps_ksz\n",
    "    del maps_tsz_ksz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_lk_baseline = np.ones(len(lk_arr_baseline[0]))\n",
    "for i in range(25):\n",
    "    comb_lk_baseline *= lk_arr_baseline[i]\n",
    "comb_lk_baseline = comb_lk_baseline/max(comb_lk_baseline)\n",
    "mean_mass_comb_baseline, errors_comb_baseline = stats.ml_params(mass_int_lk_baseline, comb_lk_baseline)      \n",
    "print(mean_mass_comb_baseline, (errors_comb_baseline[0]+errors_comb_baseline[1])/2)    \n",
    "    \n",
    "    \n",
    "comb_lk_tsz = np.ones(len(lk_arr_tsz[0]))\n",
    "for i in range(25):\n",
    "    comb_lk_tsz *= lk_arr_tsz[i]\n",
    "comb_lk_tsz = comb_lk_tsz/max(comb_lk_tsz)\n",
    "mean_mass_comb_tsz, errors_comb_tsz = stats.ml_params(mass_int_lk_tsz, comb_lk_tsz)      \n",
    "print(mean_mass_comb_tsz, (errors_comb_tsz[0]+errors_comb_tsz[1])/2) \n",
    "\n",
    "comb_lk_ksz = np.ones(len(lk_arr_ksz[0]))\n",
    "for i in range(25):\n",
    "    comb_lk_ksz *= lk_arr_ksz[i]\n",
    "comb_lk_ksz = comb_lk_ksz/max(comb_lk_ksz)\n",
    "mean_mass_comb_ksz, errors_comb_ksz = stats.ml_params(mass_int_lk_ksz, comb_lk_ksz)      \n",
    "print(mean_mass_comb_ksz, (errors_comb_ksz[0]+errors_comb_ksz[1])/2) \n",
    "      \n",
    "comb_lk_tsz_ksz = np.ones(len(lk_arr_tsz_ksz[0]))\n",
    "for i in range(25):\n",
    "    comb_lk_tsz_ksz *= lk_arr_tsz_ksz[i]\n",
    "comb_lk_tsz_ksz = comb_lk_tsz_ksz/max(comb_lk_tsz_ksz)\n",
    "mean_mass_comb_tsz_ksz, errors_comb_tsz_ksz = stats.ml_params(mass_int_lk_tsz_ksz, comb_lk_tsz_ksz)      \n",
    "print(mean_mass_comb_tsz_ksz, (errors_comb_tsz_ksz[0]+errors_comb_tsz_ksz[1])/2)       \n",
    "\n",
    "comb_lk_tsz_corrected = np.ones(len(lk_arr_tsz_corrected[0]))\n",
    "for i in range(25):\n",
    "    comb_lk_tsz_corrected *= lk_arr_tsz_corrected[i]\n",
    "comb_lk_tsz_corrected = comb_lk_tsz_corrected/max(comb_lk_tsz_corrected)\n",
    "mean_mass_comb_tsz_corrected, errors_comb_tsz_corrected = stats.ml_params(mass_int_lk_tsz_corrected, comb_lk_tsz_corrected)      \n",
    "print(mean_mass_comb_tsz_corrected, (errors_comb_tsz_corrected[0]+errors_comb_tsz_corrected[1])/2) \n",
    "\n",
    "comb_lk_tsz_ksz_corrected = np.ones(len(lk_arr_tsz_ksz_corrected[0]))\n",
    "for i in range(25):\n",
    "    comb_lk_tsz_ksz_corrected *= lk_arr_tsz_ksz_corrected[i]\n",
    "comb_lk_tsz_ksz_corrected = comb_lk_tsz_ksz_corrected/max(comb_lk_tsz_ksz_corrected)\n",
    "mean_mass_comb_tsz_ksz_corrected, errors_comb_tsz_ksz_corrected = stats.ml_params(mass_int_lk_tsz_ksz_corrected, comb_lk_tsz_ksz_corrected)      \n",
    "print(mean_mass_comb_tsz_ksz_corrected, (errors_comb_tsz_ksz_corrected[0]+errors_comb_tsz_ksz_corrected[1])/2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig, ax = plt.subplots()\n",
    "best_fit_arr = [mass_arr_baseline, mass_arr_tsz, mass_arr_ksz, mass_arr_tsz_ksz, mass_arr_tsz_corrected, mass_arr_tsz_ksz_corrected]\n",
    "centers_baseline = np.asarray(mass_arr_baseline)-M200c*1e-14\n",
    "centers_tsz = np.asarray(mass_arr_tsz)-M200c*1e-14\n",
    "centers_ksz = np.asarray(mass_arr_ksz)-M200c*1e-14\n",
    "centers_tsz_ksz = np.asarray(mass_arr_tsz_ksz)-M200c*1e-14\n",
    "centers_tsz_corrected = np.asarray(mass_arr_tsz_corrected)-M200c*1e-14\n",
    "centers_tsz_ksz_corrected = np.asarray(mass_arr_tsz_ksz_corrected)-M200c*1e-14\n",
    "centers = [centers_baseline, centers_tsz, centers_ksz, centers_tsz_ksz, centers_tsz_corrected, centers_tsz_ksz_corrected]\n",
    "width_arr = [error_arr_baseline, error_arr_tsz, error_arr_ksz, error_arr_tsz_ksz, error_arr_tsz_corrected, error_arr_tsz_ksz_corrected]\n",
    "color_arr = ['black', 'blue', 'red', 'darkviolet', 'deepskyblue', 'magenta']\n",
    "for cntr, (center, widthval, colorval) in enumerate(zip(centers, width_arr, color_arr )):\n",
    "    yval = [cntr for i in range(len(mass_arr_baseline))]\n",
    "    plt.errorbar(center, yval, xerr = widthval, color = colorval, marker = 'o', ls = '', alpha = 0.15)\n",
    "plt.errorbar(mean_mass_comb_baseline-M200c*1e-14, 0, xerr = (errors_comb_baseline[0]+errors_comb_baseline[1])/2, color = 'black', marker = 'o', ls = '')\n",
    "plt.errorbar(mean_mass_comb_tsz-M200c*1e-14, 1, xerr = (errors_comb_tsz[0]+errors_tsz[1])/2, color = 'blue', marker = 'o', ls = '')\n",
    "plt.errorbar(mean_mass_comb_ksz-M200c*1e-14, 2, xerr = (errors_comb_ksz[0]+errors_ksz[1])/2, color = 'red', marker = 'o', ls = '')\n",
    "plt.errorbar(mean_mass_comb_tsz_ksz-M200c*1e-14, 3, xerr = (errors_comb_tsz_ksz[0]+errors_tsz_ksz[1])/2, color = 'darkviolet', marker = 'o', ls = '')\n",
    "plt.errorbar(mean_mass_comb_tsz_corrected-M200c*1e-14, 4, xerr = (errors_comb_tsz_corrected[0]+errors_tsz_corrected[1])/2, color = 'deepskyblue', marker = 'o', ls = '')\n",
    "plt.errorbar(mean_mass_comb_tsz_ksz_corrected-M200c*1e-14, 5, xerr = (errors_comb_tsz_ksz_corrected[0]+errors_tsz_ksz_corrected[1])/2, color = 'magenta', marker = 'o', ls = '')\n",
    "plt.axvline(0, color = 'green', ls = '--')\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.axes.yaxis.set_ticks([])\n",
    "plt.text(0.42, 0, 'baseline', fontsize = 15)\n",
    "plt.text(0.42, 1, 'with tSZ', fontsize = 15)\n",
    "plt.text(0.42, 2, 'with kSZ', fontsize = 15)\n",
    "plt.text(0.42, 3, 'with tSZ and kSZ', fontsize = 15)\n",
    "plt.text(0.42, 4, 'tSZ corrected', fontsize = 15)\n",
    "plt.text(0.42, 5, 'tSZ and kSZ corrected', fontsize = 15)\n",
    "plt.xlabel(r'$M_{lens}-M_{true}$', fontsize = 15)\n",
    "plt.savefig('/Volumes/Extreme_SSD/codes/master_thesis/code/figures/foreground_bias.eps', dpi = 200., bbox_inches = 'tight', pad_inches = 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
